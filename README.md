# GDELT Cleaning & Embedding Pipeline

> State-of-the-art semantic indexing of the GDELT Global Database of Events, Language, and Tone.

## ğŸ¯ Mission

GDELT is the world's largest open dataset of global events, updated every 15 minutes. But raw GDELT data is noisy, redundant, and lacks semantic structure. This project transforms raw GDELT into a **semantically searchable, embedding-indexed knowledge base** â€” enabling millisecond retrieval of geopolitical events by meaning, not just keywords.

**We do NOT duplicate GDELT.** We clean, embed, and index â€” creating lightweight semantic layers that point back to source data.

## ğŸ—ï¸ Architecture

```
GDELT API (Events 2.0 + GKG)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ingestion Layer     â”‚  â† api-ingestion-framework (Databricks)
â”‚  (Raw â†’ Bronze)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cleaning Pipeline   â”‚  â† THIS REPO
â”‚  â€¢ Deduplication     â”‚
â”‚  â€¢ Entity Resolution â”‚
â”‚  â€¢ Text Normalizationâ”‚
â”‚  â€¢ Quality Scoring   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embedding Layer     â”‚
â”‚  â€¢ Modern Transformerâ”‚
â”‚    Embeddings        â”‚
â”‚  â€¢ NOT USEv4 (too    â”‚
â”‚    weak for prod)    â”‚
â”‚  â€¢ Candidate models: â”‚
â”‚    - BGE-M3          â”‚
â”‚    - E5-large-v2     â”‚
â”‚    - Cohere embed-v3 â”‚
â”‚    - Voyage-3        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Index & Retrieval   â”‚
â”‚  â€¢ Vector DB         â”‚
â”‚    (FAISS / Qdrant / â”‚
â”‚     Databricks VS)   â”‚
â”‚  â€¢ Semantic Search   â”‚
â”‚  â€¢ Temporal Filteringâ”‚
â”‚  â€¢ Geo-spatial Index â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”‘ Design Principles

1. **No data duplication** â€” We index and embed, we don't clone GDELT. Embeddings + metadata = lightweight.
2. **Databricks-native** â€” Everything runs on Databricks. Delta Lake, Unity Catalog, MLflow for model tracking.
3. **Continuous pipeline** â€” Not a one-shot script. Cron-based ingestion every 15 minutes, incremental embedding.
4. **Embedding model agnostic** â€” Pluggable embedding layer. Start with open-source, benchmark, upgrade.
5. **Production-grade** â€” Tests, monitoring, retry logic, data quality checks.

## ğŸ“‹ Roadmap

### Phase 1: Data Cleaning (Current)
- [ ] Connect to api-ingestion-framework Bronze layer
- [ ] Deduplication pipeline (GDELT has ~15-30% redundancy across updates)
- [ ] Entity normalization (actor codes â†’ canonical names)
- [ ] Text cleaning for source URLs and event descriptions
- [ ] Data quality scoring (completeness, consistency, freshness)
- [ ] Output: Clean Silver-layer Delta tables

### Phase 2: Embedding & Indexing
- [ ] Benchmark embedding models on GDELT event descriptions
- [ ] Build embedding pipeline (batch + incremental)
- [ ] Vector DB setup (evaluate Databricks Vector Search vs. self-hosted)
- [ ] Temporal + geo-spatial index layers
- [ ] Output: Searchable vector index with metadata filters

### Phase 3: Retrieval API
- [ ] Semantic search endpoint
- [ ] Temporal range queries ("events in Syria last 72h")
- [ ] Actor-based queries ("all Russia-Ukraine interactions")
- [ ] Goldstein scale filtering (conflict intensity)
- [ ] Integration with Vector News AI platform

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| Orchestration | Databricks Workflows / Delta Live Tables |
| Storage | Delta Lake (Unity Catalog) |
| Compute | Databricks (Spark + single-node for embedding) |
| Embeddings | TBD â€” benchmarking BGE-M3, E5-large-v2, Voyage-3 |
| Vector DB | TBD â€” evaluating Databricks VS, Qdrant, FAISS |
| Monitoring | MLflow + Great Expectations |
| Language | Python 3.11+ |

## ğŸ“Š Why Not USEv4?

We evaluated Google's Universal Sentence Encoder v4 for GDELT embeddings. Results:
- Poor semantic separation for geopolitical event types
- Weak on multilingual content (GDELT is global)
- Embedding dimension (512) insufficient for fine-grained similarity
- Modern alternatives (BGE-M3, E5) outperform by 15-25% on MTEB benchmarks

â†’ We embed with state-of-the-art models from day one.

## ğŸš€ Getting Started

```bash
# Clone
git clone https://github.com/HedgingHarald/gdelt-cleaning-pipeline.git
cd gdelt-cleaning-pipeline

# Setup (requires Databricks CLI configured)
pip install -e ".[dev]"

# Run cleaning pipeline
python -m src.pipeline.run --source events_v2 --mode incremental
```

## ğŸ“ Project Structure

```
gdelt-cleaning-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ databricks.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cleaning/          # Dedup, normalization, quality
â”‚   â”œâ”€â”€ embedding/         # Model loading, batch embedding
â”‚   â”œâ”€â”€ indexing/          # Vector DB operations
â”‚   â””â”€â”€ pipeline/          # Orchestration, scheduling
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ cleaning.yaml      # Cleaning rules & thresholds
â”‚   â””â”€â”€ embedding.yaml     # Model config & hyperparams
â”œâ”€â”€ notebooks/             # Databricks notebooks
â”œâ”€â”€ tests/
â””â”€â”€ benchmarks/            # Embedding model comparisons
```

## ğŸ“ License

MIT

---

*Part of the Vector News AI ecosystem. Built for speed, accuracy, and scale.*
